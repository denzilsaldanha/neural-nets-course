{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2_part3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denzilsaldanha/neural-nets-course/blob/master/hw2_part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L32NiAdGwt1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download hw2 archive and setup environment\n",
        "!wget https://www.cse.unsw.edu.au/~cs9444/19T3/hw2/hw2.zip\n",
        "!unzip hw2.zip\n",
        "!mv -t . hw2/data hw2/imdb_dataloader.py hw2/.vector_cache\n",
        "!rm -rf hw2 hw2.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkBZpCrJv0o5",
        "colab_type": "text"
      },
      "source": [
        "BI DIRECTIONAL GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjSgd5iZ0OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p data/imdb/aclImdb/train/{pos,neg}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSij9jOJaCn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -a data/imdb/aclImdb/train/neg data/imdb/aclImdb/all\n",
        "!cp -a data/imdb/aclImdb/train/pos data/imdb/aclImdb/all\n",
        "!cp -a data/imdb/aclImdb/dev/neg data/imdb/aclImdb/all\n",
        "!cp -a data/imdb/aclImdb/dev/pos data/imdb/aclImdb/all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmGTTOJ4R9Mq",
        "colab_type": "code",
        "outputId": "13115a4a-2c76-48eb-dfba-a585f27ddffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as tnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as topti\n",
        "from torchtext import data\n",
        "from torchtext.vocab import GloVe\n",
        "from imdb_dataloader import IMDB\n",
        "import string\n",
        "\n",
        "\n",
        "# Class for creating the neural network.\n",
        "class Network(tnn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        embedding_dim = 50\n",
        "        hidden_dim = 220\n",
        "        output_dim = 1\n",
        "        n_layers = 2\n",
        "        bidirectional = True\n",
        "        dropout = 0.6\n",
        "        self.gru = tnn.GRU(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,batch_first = True)\n",
        "      \n",
        "        \n",
        "        self.fc = tnn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.fc1 = tnn.Linear(hidden_dim,1)\n",
        "        self.dropout = tnn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, input, lengths):\n",
        "      # print(input.size())\n",
        "      #text = [sent len, batch size]\n",
        "      # input = input.permute(0,2,1)\n",
        "      embedded = input\n",
        "      # embedded = self.dropout(self.embedding(text))\n",
        "      \n",
        "      #embedded = [sent len, batch size, emb dim]\n",
        "      \n",
        "      #pack sequence\n",
        "      packed_embedded = tnn.utils.rnn.pack_padded_sequence(embedded, lengths,batch_first = True)\n",
        "      \n",
        "      # packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "      packed_output, hidden = self.gru(packed_embedded)\n",
        "      #unpack sequence\n",
        "      output, output_lengths = tnn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "      #output = [sent len, batch size, hid dim * num directions]\n",
        "      #output over padding tokens are zero tensors\n",
        "      \n",
        "      #hidden = [num layers * num directions, batch size, hid dim]\n",
        "      #cell = [num layers * num directions, batch size, hid dim]\n",
        "      \n",
        "      #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "      #and apply dropout\n",
        "      \n",
        "      hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "              \n",
        "      #hidden = [batch size, hid dim * num directions]\n",
        "          \n",
        "      output=   self.fc(hidden) \n",
        "      output = self.fc1(output)\n",
        "      # output = self.fc2(output)\n",
        "      # output = self.fc3(output)\n",
        "      # output = self.fc4(output)\n",
        "     \n",
        "\n",
        "      output = output[:,-1]\n",
        "      return output\n",
        "        \n",
        "\n",
        "\n",
        "class PreProcessing():\n",
        "    def pre(x):\n",
        "        \"\"\"Called after tokenization\"\"\"\n",
        "        # print(x)\n",
        "        #Remove Punctuations\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        tokens = [w.translate(table) for w in x]\n",
        "        #Remove numbers\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        #Removing common stop words\n",
        "        # TO DO  - Would removing stop words make a difference ? \n",
        "        # stop_words = ['br','a','about','after','again','against','ain','all','am','an','and','any','are','aren','arent','as','at','be','because','been','before','being','both','but','by','can','couldn','couldnt','d','did','didn','didnt','do','does','doesn','doesnt','doing','don','dont','down','during','each','few','for','from','further','had','hadn','hadnt','has','hasn','hasnt','have','haven','havent','having','he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','isn','isnt','it','its','its','itself','just','ll','m','ma','me','mightn','mightnt','more','most','mustn','mustnt','my','myself','needn','neednt','no','nor','not','now','o','of','off','on','once','only','or','other','our','ours','ourselves','out','over','own','re','s','same','shan','shant','she','shes','should','shouldve','shouldn','shouldnt','so','some','such','t','than','that','thatll','the','their','theirs','them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','ve','very','was','wasn','wasnt','we','were','weren','werent','what','when','where','which','while','who','whom','why','will','with','won','wont','wouldn','y','you','youd','youll','youre','youve','your','yours','yourself','yourselves','could','hed','hell','hes','heres','hows','id','ill','im','ive','lets','shed','shell','thats','theres','theyd','theyll','theyre','theyve','wed','well','were','weve','whats','whens','whos','whys','would'] \n",
        "        stop_words = ['br']\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        #removing alphabets\n",
        "        x = [word for word in tokens if len(word) > 1]\n",
        "        return x\n",
        "\n",
        "    def post(batch, vocab):\n",
        "        \"\"\"Called after numericalization but prior to vectorization\"\"\"\n",
        "      \n",
        "        \n",
        "        return batch, vocab\n",
        "\n",
        "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=None)\n",
        "\n",
        "\n",
        "def lossFunc():\n",
        "    \"\"\"\n",
        "    Define a loss function appropriate for the above networks that will\n",
        "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
        "    \"\"\"\n",
        "    return tnn.BCEWithLogitsLoss()\n",
        "\n",
        "def main():\n",
        "    # Use a GPU if available, as it should be faster.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device: \" + str(device))\n",
        "\n",
        "    # Load the training dataset, and create a data loader to generate a batch.\n",
        "    textField = PreProcessing.text_field\n",
        "    labelField = data.Field(sequential=False)\n",
        "\n",
        "    train, dev = IMDB.splits(textField, labelField, train=\"all\", validation=\"dev\")\n",
        "\n",
        "    textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
        "    labelField.build_vocab(train, dev)\n",
        "    # print(\"Input_DIM = \" , len(textField.vocab))\n",
        "    # print(\"pad_idx = \" ,  textField.vocab.stoi[textField.pad_token])\n",
        "\n",
        "    trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
        "                                                         sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
        "    \n",
        "    net = Network().to(device)\n",
        "    criterion =lossFunc()\n",
        "    optimiser = topti.Adam(net.parameters(), lr=0.001)  # Minimise the loss using the Adam algorithm.\n",
        "\n",
        "    for epoch in range(15):\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(trainLoader):\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "\n",
        "            labels -= 1\n",
        "\n",
        "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
        "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            # Forward pass through the network.\n",
        "            output = net(inputs, length)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            # Calculate gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Minimise the loss according to the gradient.\n",
        "            optimiser.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 32 == 31:\n",
        "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
        "                running_loss = 0\n",
        "\n",
        "    num_correct = 0\n",
        "\n",
        "    # Save mode\n",
        "    torch.save(net.state_dict(), \"./model.pth\")\n",
        "    print(\"Saved model\")\n",
        "\n",
        "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
        "    # computations and reduce memory usage.\n",
        "    with torch.no_grad():\n",
        "        for batch in testLoader:\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "            net.eval()\n",
        "            labels -= 1\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = torch.sigmoid(net(inputs, length))\n",
        "            predicted = torch.round(outputs)\n",
        "\n",
        "            num_correct += torch.sum(labels == predicted).item()\n",
        "\n",
        "    accuracy = 100 * num_correct / len(dev)\n",
        "\n",
        "    print(f\"Classification accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399668/400000 [00:09<00:00, 41875.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1, Batch:   32, Loss: 0.474\n",
            "Epoch:  1, Batch:   64, Loss: 0.454\n",
            "Epoch:  1, Batch:   96, Loss: 0.410\n",
            "Epoch:  1, Batch:  128, Loss: 0.425\n",
            "Epoch:  1, Batch:  160, Loss: 0.445\n",
            "Epoch:  1, Batch:  192, Loss: 0.405\n",
            "Epoch:  1, Batch:  224, Loss: 0.422\n",
            "Epoch:  1, Batch:  256, Loss: 0.373\n",
            "Epoch:  2, Batch:   32, Loss: 0.348\n",
            "Epoch:  2, Batch:   64, Loss: 0.360\n",
            "Epoch:  2, Batch:   96, Loss: 0.319\n",
            "Epoch:  2, Batch:  128, Loss: 0.302\n",
            "Epoch:  2, Batch:  160, Loss: 0.333\n",
            "Epoch:  2, Batch:  192, Loss: 0.288\n",
            "Epoch:  2, Batch:  224, Loss: 0.285\n",
            "Epoch:  2, Batch:  256, Loss: 0.246\n",
            "Epoch:  3, Batch:   32, Loss: 0.265\n",
            "Epoch:  3, Batch:   64, Loss: 0.301\n",
            "Epoch:  3, Batch:   96, Loss: 0.273\n",
            "Epoch:  3, Batch:  128, Loss: 0.260\n",
            "Epoch:  3, Batch:  160, Loss: 0.256\n",
            "Epoch:  3, Batch:  192, Loss: 0.270\n",
            "Epoch:  3, Batch:  224, Loss: 0.241\n",
            "Epoch:  3, Batch:  256, Loss: 0.256\n",
            "Epoch:  4, Batch:   32, Loss: 0.269\n",
            "Epoch:  4, Batch:   64, Loss: 0.206\n",
            "Epoch:  4, Batch:   96, Loss: 0.256\n",
            "Epoch:  4, Batch:  128, Loss: 0.241\n",
            "Epoch:  4, Batch:  160, Loss: 0.241\n",
            "Epoch:  4, Batch:  192, Loss: 0.256\n",
            "Epoch:  4, Batch:  224, Loss: 0.227\n",
            "Epoch:  4, Batch:  256, Loss: 0.241\n",
            "Epoch:  5, Batch:   32, Loss: 0.230\n",
            "Epoch:  5, Batch:   64, Loss: 0.210\n",
            "Epoch:  5, Batch:   96, Loss: 0.208\n",
            "Epoch:  5, Batch:  128, Loss: 0.225\n",
            "Epoch:  5, Batch:  160, Loss: 0.244\n",
            "Epoch:  5, Batch:  192, Loss: 0.218\n",
            "Epoch:  5, Batch:  224, Loss: 0.227\n",
            "Epoch:  5, Batch:  256, Loss: 0.224\n",
            "Epoch:  6, Batch:   32, Loss: 0.210\n",
            "Epoch:  6, Batch:   64, Loss: 0.207\n",
            "Epoch:  6, Batch:   96, Loss: 0.193\n",
            "Epoch:  6, Batch:  128, Loss: 0.174\n",
            "Epoch:  6, Batch:  160, Loss: 0.169\n",
            "Epoch:  6, Batch:  192, Loss: 0.212\n",
            "Epoch:  6, Batch:  224, Loss: 0.210\n",
            "Epoch:  6, Batch:  256, Loss: 0.214\n",
            "Epoch:  7, Batch:   32, Loss: 0.182\n",
            "Epoch:  7, Batch:   64, Loss: 0.189\n",
            "Epoch:  7, Batch:   96, Loss: 0.187\n",
            "Epoch:  7, Batch:  128, Loss: 0.191\n",
            "Epoch:  7, Batch:  160, Loss: 0.170\n",
            "Epoch:  7, Batch:  192, Loss: 0.174\n",
            "Epoch:  7, Batch:  224, Loss: 0.171\n",
            "Epoch:  7, Batch:  256, Loss: 0.176\n",
            "Epoch:  8, Batch:   32, Loss: 0.155\n",
            "Epoch:  8, Batch:   64, Loss: 0.184\n",
            "Epoch:  8, Batch:   96, Loss: 0.143\n",
            "Epoch:  8, Batch:  128, Loss: 0.156\n",
            "Epoch:  8, Batch:  160, Loss: 0.167\n",
            "Epoch:  8, Batch:  192, Loss: 0.149\n",
            "Epoch:  8, Batch:  224, Loss: 0.158\n",
            "Epoch:  8, Batch:  256, Loss: 0.185\n",
            "Epoch:  9, Batch:   32, Loss: 0.113\n",
            "Epoch:  9, Batch:   64, Loss: 0.130\n",
            "Epoch:  9, Batch:   96, Loss: 0.140\n",
            "Epoch:  9, Batch:  128, Loss: 0.130\n",
            "Epoch:  9, Batch:  160, Loss: 0.137\n",
            "Epoch:  9, Batch:  192, Loss: 0.143\n",
            "Epoch:  9, Batch:  224, Loss: 0.135\n",
            "Epoch:  9, Batch:  256, Loss: 0.124\n",
            "Epoch: 10, Batch:   32, Loss: 0.099\n",
            "Epoch: 10, Batch:   64, Loss: 0.110\n",
            "Epoch: 10, Batch:   96, Loss: 0.146\n",
            "Epoch: 10, Batch:  128, Loss: 0.104\n",
            "Epoch: 10, Batch:  160, Loss: 0.149\n",
            "Epoch: 10, Batch:  192, Loss: 0.101\n",
            "Epoch: 10, Batch:  224, Loss: 0.121\n",
            "Epoch: 10, Batch:  256, Loss: 0.116\n",
            "Epoch: 11, Batch:   32, Loss: 0.081\n",
            "Epoch: 11, Batch:   64, Loss: 0.085\n",
            "Epoch: 11, Batch:   96, Loss: 0.083\n",
            "Epoch: 11, Batch:  128, Loss: 0.089\n",
            "Epoch: 11, Batch:  160, Loss: 0.088\n",
            "Epoch: 11, Batch:  192, Loss: 0.079\n",
            "Epoch: 11, Batch:  224, Loss: 0.087\n",
            "Epoch: 11, Batch:  256, Loss: 0.093\n",
            "Epoch: 12, Batch:   32, Loss: 0.055\n",
            "Epoch: 12, Batch:   64, Loss: 0.048\n",
            "Epoch: 12, Batch:   96, Loss: 0.072\n",
            "Epoch: 12, Batch:  128, Loss: 0.074\n",
            "Epoch: 12, Batch:  160, Loss: 0.087\n",
            "Epoch: 12, Batch:  192, Loss: 0.066\n",
            "Epoch: 12, Batch:  224, Loss: 0.075\n",
            "Epoch: 12, Batch:  256, Loss: 0.073\n",
            "Epoch: 13, Batch:   32, Loss: 0.037\n",
            "Epoch: 13, Batch:   64, Loss: 0.028\n",
            "Epoch: 13, Batch:   96, Loss: 0.043\n",
            "Epoch: 13, Batch:  128, Loss: 0.051\n",
            "Epoch: 13, Batch:  160, Loss: 0.053\n",
            "Epoch: 13, Batch:  192, Loss: 0.045\n",
            "Epoch: 13, Batch:  224, Loss: 0.059\n",
            "Epoch: 13, Batch:  256, Loss: 0.049\n",
            "Epoch: 14, Batch:   32, Loss: 0.028\n",
            "Epoch: 14, Batch:   64, Loss: 0.039\n",
            "Epoch: 14, Batch:   96, Loss: 0.058\n",
            "Epoch: 14, Batch:  128, Loss: 0.031\n",
            "Epoch: 14, Batch:  160, Loss: 0.047\n",
            "Epoch: 14, Batch:  192, Loss: 0.033\n",
            "Epoch: 14, Batch:  224, Loss: 0.030\n",
            "Epoch: 14, Batch:  256, Loss: 0.051\n",
            "Epoch: 15, Batch:   32, Loss: 0.017\n",
            "Epoch: 15, Batch:   64, Loss: 0.021\n",
            "Epoch: 15, Batch:   96, Loss: 0.044\n",
            "Epoch: 15, Batch:  128, Loss: 0.049\n",
            "Epoch: 15, Batch:  160, Loss: 0.027\n",
            "Epoch: 15, Batch:  192, Loss: 0.030\n",
            "Epoch: 15, Batch:  224, Loss: 0.067\n",
            "Epoch: 15, Batch:  256, Loss: 0.043\n",
            "Saved model\n",
            "Classification accuracy: 98.78361075544174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lPedkFG0OCT",
        "colab_type": "text"
      },
      "source": [
        "GRU WITHOUT STOP WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHqvc8DW0Nku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as tnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as topti\n",
        "from torchtext import data\n",
        "from torchtext.vocab import GloVe\n",
        "from imdb_dataloader import IMDB\n",
        "import string\n",
        "\n",
        "\n",
        "# Class for creating the neural network.\n",
        "class Network(tnn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "  #       n_filters = 100\n",
        "  #       filter_sizes = [3,4,5]\n",
        "  #       output_dim = 1\n",
        "  #       dropout = 0.5\n",
        "  #       self.embedding = tnn.Embedding(133418, 100, padding_idx = 1)\n",
        "        \n",
        "  #       self.convs = tnn.ModuleList([\n",
        "  #                                   tnn.Conv1d(in_channels = 50, \n",
        "  #                                             out_channels = 100, \n",
        "  #                                             kernel_size = fs)\n",
        "  #                                   for fs in filter_sizes\n",
        "  #                                   ])\n",
        "        \n",
        "  #       self.fc = tnn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "  #       self.dropout = tnn.Dropout(dropout)\n",
        "        \n",
        "  # def forward(self, input,length):\n",
        "      \n",
        "  #     #text = [batch size, sent len]\n",
        "  #     print(input)\n",
        "  #     embedded = input\n",
        "              \n",
        "  #     #embedded = [batch size, sent len, emb dim]\n",
        "      \n",
        "  #     embedded = embedded.permute(0, 2, 1)\n",
        "      \n",
        "  #     #embedded = [batch size, emb dim, sent len]\n",
        "      \n",
        "  #     conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "          \n",
        "  #     #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "      \n",
        "  #     pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "      \n",
        "  #     #pooled_n = [batch size, n_filters]\n",
        "      \n",
        "  #     cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "  #     output = self.fc(cat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        embedding_dim = 50\n",
        "        hidden_dim = 256\n",
        "        output_dim = 1\n",
        "        n_layers = 2\n",
        "        bidirectional = True\n",
        "        dropout = 0.5\n",
        "\n",
        "        # self.embedding = tnn.Embedding(133418, embedding_dim, padding_idx = 1).float()\n",
        "        self.gru = tnn.GRU(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,batch_first = True)\n",
        "        # self.rnn = tnn.LSTM(embedding_dim, \n",
        "        #                    hidden_dim, \n",
        "        #                    num_layers=n_layers, \n",
        "        #                    bidirectional=bidirectional, \n",
        "        #                    dropout=dropout,batch_first = True)\n",
        "        \n",
        "        self.fc = tnn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = tnn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, input, lengths):\n",
        "      # print(input.size())\n",
        "      #text = [sent len, batch size]\n",
        "      # input = input.permute(0,2,1)\n",
        "      embedded = input\n",
        "      # embedded = self.dropout(self.embedding(text))\n",
        "      \n",
        "      #embedded = [sent len, batch size, emb dim]\n",
        "      \n",
        "      #pack sequence\n",
        "      packed_embedded = tnn.utils.rnn.pack_padded_sequence(embedded, lengths,batch_first = True)\n",
        "      \n",
        "      # packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "      packed_output, hidden = self.gru(packed_embedded)\n",
        "      #unpack sequence\n",
        "      output, output_lengths = tnn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "      #output = [sent len, batch size, hid dim * num directions]\n",
        "      #output over padding tokens are zero tensors\n",
        "      \n",
        "      #hidden = [num layers * num directions, batch size, hid dim]\n",
        "      #cell = [num layers * num directions, batch size, hid dim]\n",
        "      \n",
        "      #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "      #and apply dropout\n",
        "      \n",
        "      hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "              \n",
        "      #hidden = [batch size, hid dim * num directions]\n",
        "          \n",
        "      output=   self.fc(hidden) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #       self.relu = tnn.ReLU()\n",
        "  #       self.max_pool =tnn.MaxPool1d(kernel_size = 4)\n",
        "  #       self.conv_1 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.conv_2 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.conv_3 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.mp = tnn.AdaptiveMaxPool1d(output_size = 1)\n",
        "  #       self.fc = tnn.Linear(50, 1)\n",
        "  # def forward(self, input, length):\n",
        "  #     \"\"\"\n",
        "  #     DO NOT MODIFY FUNCTION SIGNATURE\n",
        "  #     TODO:\n",
        "  #     Create the forward pass through the network.\n",
        "  #     \"\"\"\n",
        "  # #        print(length)\n",
        "  # #        print(input.shape)\n",
        "  #     #batch_size, seq_length, ip_dim = input.shape\n",
        "  #     #x = input.view([batch_size,ip_dim, seq_length])\n",
        "  #     x = input.permute(0,2,1)\n",
        "  #     x= self.conv_1(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x= self.max_pool(x)\n",
        "  #     x= self.conv_2(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x= self.max_pool(x) \n",
        "  #     x= self.conv_3(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x = self.mp(x)\n",
        "  #     #x= x.view(batch_size, -1)\n",
        "  #     x = x.squeeze(-1)\n",
        "  #     output = self.fc(x)\n",
        "      output = output[:,-1]\n",
        "      return output\n",
        "        \n",
        "\n",
        "\n",
        "class PreProcessing():\n",
        "    def pre(x):\n",
        "        \"\"\"Called after tokenization\"\"\"\n",
        "        # print(x)\n",
        "        #Remove Punctuations\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        tokens = [w.translate(table) for w in x]\n",
        "        #Remove numbers\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        #Removing common stop words\n",
        "        # TO DO  - Would removing stop words make a difference ? \n",
        "        stop_words = [\"br\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "        # stop_words = ['br']\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        #removing alphabets\n",
        "        x = [word for word in tokens if len(word) > 1]\n",
        "        return x\n",
        "\n",
        "    def post(batch, vocab):\n",
        "        \"\"\"Called after numericalization but prior to vectorization\"\"\"\n",
        "      \n",
        "        \n",
        "        return batch, vocab\n",
        "\n",
        "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=None)\n",
        "\n",
        "\n",
        "def lossFunc():\n",
        "    \"\"\"\n",
        "    Define a loss function appropriate for the above networks that will\n",
        "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
        "    \"\"\"\n",
        "    return tnn.BCEWithLogitsLoss()\n",
        "\n",
        "def main():\n",
        "    # Use a GPU if available, as it should be faster.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device: \" + str(device))\n",
        "\n",
        "    # Load the training dataset, and create a data loader to generate a batch.\n",
        "    textField = PreProcessing.text_field\n",
        "    labelField = data.Field(sequential=False)\n",
        "\n",
        "    train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
        "\n",
        "    textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
        "    labelField.build_vocab(train, dev)\n",
        "    print(\"Input_DIM = \" , len(textField.vocab))\n",
        "    print(\"pad_idx = \" ,  textField.vocab.stoi[textField.pad_token])\n",
        "\n",
        "    trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
        "                                                         sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
        "    \n",
        "    net = Network().to(device)\n",
        "    criterion =lossFunc()\n",
        "    optimiser = topti.Adam(net.parameters(), lr=0.001)  # Minimise the loss using the Adam algorithm.\n",
        "\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(trainLoader):\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "\n",
        "            labels -= 1\n",
        "\n",
        "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
        "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            # Forward pass through the network.\n",
        "            output = net(inputs, length)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            # Calculate gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Minimise the loss according to the gradient.\n",
        "            optimiser.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 32 == 31:\n",
        "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
        "                running_loss = 0\n",
        "\n",
        "    num_correct = 0\n",
        "\n",
        "    # Save mode\n",
        "    torch.save(net.state_dict(), \"./model.pth\")\n",
        "    print(\"Saved model\")\n",
        "\n",
        "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
        "    # computations and reduce memory usage.\n",
        "    with torch.no_grad():\n",
        "        for batch in testLoader:\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "\n",
        "            labels -= 1\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = torch.sigmoid(net(inputs, length))\n",
        "            predicted = torch.round(outputs)\n",
        "\n",
        "            num_correct += torch.sum(labels == predicted).item()\n",
        "\n",
        "    accuracy = 100 * num_correct / len(dev)\n",
        "\n",
        "    print(f\"Classification accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQsAh1SIvrXe",
        "colab_type": "text"
      },
      "source": [
        "LSTM WITH STOP WORDS REMOVED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW75LrQ-E8h1",
        "colab_type": "code",
        "outputId": "78eda7f3-91de-4472-8fd5-b52e5e64e195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as tnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as topti\n",
        "from torchtext import data\n",
        "from torchtext.vocab import GloVe\n",
        "from imdb_dataloader import IMDB\n",
        "import string\n",
        "\n",
        "\n",
        "# Class for creating the neural network.\n",
        "class Network(tnn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "  #       n_filters = 100\n",
        "  #       filter_sizes = [3,4,5]\n",
        "  #       output_dim = 1\n",
        "  #       dropout = 0.5\n",
        "  #       self.embedding = tnn.Embedding(133418, 100, padding_idx = 1)\n",
        "        \n",
        "  #       self.convs = tnn.ModuleList([\n",
        "  #                                   tnn.Conv1d(in_channels = 50, \n",
        "  #                                             out_channels = 100, \n",
        "  #                                             kernel_size = fs)\n",
        "  #                                   for fs in filter_sizes\n",
        "  #                                   ])\n",
        "        \n",
        "  #       self.fc = tnn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "  #       self.dropout = tnn.Dropout(dropout)\n",
        "        \n",
        "  # def forward(self, input,length):\n",
        "      \n",
        "  #     #text = [batch size, sent len]\n",
        "  #     print(input)\n",
        "  #     embedded = input\n",
        "              \n",
        "  #     #embedded = [batch size, sent len, emb dim]\n",
        "      \n",
        "  #     embedded = embedded.permute(0, 2, 1)\n",
        "      \n",
        "  #     #embedded = [batch size, emb dim, sent len]\n",
        "      \n",
        "  #     conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "          \n",
        "  #     #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "      \n",
        "  #     pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "      \n",
        "  #     #pooled_n = [batch size, n_filters]\n",
        "      \n",
        "  #     cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "  #     output = self.fc(cat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        embedding_dim = 50\n",
        "        hidden_dim = 256\n",
        "        output_dim = 1\n",
        "        n_layers = 2\n",
        "        bidirectional = True\n",
        "        dropout = 0.5\n",
        "\n",
        "        # self.embedding = tnn.Embedding(133418, embedding_dim, padding_idx = 1).float()\n",
        "        \n",
        "        self.rnn = tnn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,batch_first = True)\n",
        "        \n",
        "        self.fc = tnn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = tnn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, input, lengths):\n",
        "      # print(input.size())\n",
        "      #text = [sent len, batch size]\n",
        "      # input = input.permute(0,2,1)\n",
        "      embedded = input\n",
        "      # embedded = self.dropout(self.embedding(text))\n",
        "      \n",
        "      #embedded = [sent len, batch size, emb dim]\n",
        "      \n",
        "      #pack sequence\n",
        "      packed_embedded = tnn.utils.rnn.pack_padded_sequence(embedded, lengths,batch_first = True)\n",
        "      \n",
        "      packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "      # packed_output, hidden = self.rnn(packed_embedded)\n",
        "      #unpack sequence\n",
        "      output, output_lengths = tnn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "      #output = [sent len, batch size, hid dim * num directions]\n",
        "      #output over padding tokens are zero tensors\n",
        "      \n",
        "      #hidden = [num layers * num directions, batch size, hid dim]\n",
        "      #cell = [num layers * num directions, batch size, hid dim]\n",
        "      \n",
        "      #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "      #and apply dropout\n",
        "      \n",
        "      hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "              \n",
        "      #hidden = [batch size, hid dim * num directions]\n",
        "          \n",
        "      output=   self.fc(hidden) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #       self.relu = tnn.ReLU()\n",
        "  #       self.max_pool =tnn.MaxPool1d(kernel_size = 4)\n",
        "  #       self.conv_1 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.conv_2 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.conv_3 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.mp = tnn.AdaptiveMaxPool1d(output_size = 1)\n",
        "  #       self.fc = tnn.Linear(50, 1)\n",
        "  # def forward(self, input, length):\n",
        "  #     \"\"\"\n",
        "  #     DO NOT MODIFY FUNCTION SIGNATURE\n",
        "  #     TODO:\n",
        "  #     Create the forward pass through the network.\n",
        "  #     \"\"\"\n",
        "  # #        print(length)\n",
        "  # #        print(input.shape)\n",
        "  #     #batch_size, seq_length, ip_dim = input.shape\n",
        "  #     #x = input.view([batch_size,ip_dim, seq_length])\n",
        "  #     x = input.permute(0,2,1)\n",
        "  #     x= self.conv_1(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x= self.max_pool(x)\n",
        "  #     x= self.conv_2(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x= self.max_pool(x) \n",
        "  #     x= self.conv_3(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x = self.mp(x)\n",
        "  #     #x= x.view(batch_size, -1)\n",
        "  #     x = x.squeeze(-1)\n",
        "  #     output = self.fc(x)\n",
        "      output = output[:,-1]\n",
        "      return output\n",
        "        \n",
        "\n",
        "\n",
        "class PreProcessing():\n",
        "    def pre(x):\n",
        "        \"\"\"Called after tokenization\"\"\"\n",
        "        # print(x)\n",
        "        #Remove Punctuations\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        tokens = [w.translate(table) for w in x]\n",
        "        #Remove numbers\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        #Removing common stop words\n",
        "        # TO DO  - Would removing stop words make a difference ? \n",
        "        stop_words = [\"br\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "        # stop_words = ['br']\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        #removing alphabets\n",
        "        x = [word for word in tokens if len(word) > 1]\n",
        "        return x\n",
        "\n",
        "    def post(batch, vocab):\n",
        "        \"\"\"Called after numericalization but prior to vectorization\"\"\"\n",
        "      \n",
        "        \n",
        "        return batch, vocab\n",
        "\n",
        "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=None)\n",
        "\n",
        "\n",
        "def lossFunc():\n",
        "    \"\"\"\n",
        "    Define a loss function appropriate for the above networks that will\n",
        "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
        "    \"\"\"\n",
        "    return tnn.BCEWithLogitsLoss()\n",
        "\n",
        "def main():\n",
        "    # Use a GPU if available, as it should be faster.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device: \" + str(device))\n",
        "\n",
        "    # Load the training dataset, and create a data loader to generate a batch.\n",
        "    textField = PreProcessing.text_field\n",
        "    labelField = data.Field(sequential=False)\n",
        "\n",
        "    train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
        "\n",
        "    textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
        "    labelField.build_vocab(train, dev)\n",
        "    print(\"Input_DIM = \" , len(textField.vocab))\n",
        "    print(\"pad_idx = \" ,  textField.vocab.stoi[textField.pad_token])\n",
        "\n",
        "    trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
        "                                                         sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
        "    \n",
        "    net = Network().to(device)\n",
        "    criterion =lossFunc()\n",
        "    optimiser = topti.Adam(net.parameters(), lr=0.001)  # Minimise the loss using the Adam algorithm.\n",
        "\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(trainLoader):\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "\n",
        "            labels -= 1\n",
        "\n",
        "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
        "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            # Forward pass through the network.\n",
        "            output = net(inputs, length)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            # Calculate gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Minimise the loss according to the gradient.\n",
        "            optimiser.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 32 == 31:\n",
        "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
        "                running_loss = 0\n",
        "\n",
        "    num_correct = 0\n",
        "\n",
        "    # Save mode\n",
        "    torch.save(net.state_dict(), \"./model.pth\")\n",
        "    print(\"Saved model\")\n",
        "\n",
        "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
        "    # computations and reduce memory usage.\n",
        "    with torch.no_grad():\n",
        "        for batch in testLoader:\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "\n",
        "            labels -= 1\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = torch.sigmoid(net(inputs, length))\n",
        "            predicted = torch.round(outputs)\n",
        "\n",
        "            num_correct += torch.sum(labels == predicted).item()\n",
        "\n",
        "    accuracy = 100 * num_correct / len(dev)\n",
        "\n",
        "    print(f\"Classification accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Input_DIM =  133295\n",
            "pad_idx =  1\n",
            "Epoch:  1, Batch:   32, Loss: 0.641\n",
            "Epoch:  1, Batch:   64, Loss: 0.649\n",
            "Epoch:  1, Batch:   96, Loss: 0.636\n",
            "Epoch:  1, Batch:  128, Loss: 0.585\n",
            "Epoch:  1, Batch:  160, Loss: 0.593\n",
            "Epoch:  1, Batch:  192, Loss: 0.580\n",
            "Epoch:  1, Batch:  224, Loss: 0.612\n",
            "Epoch:  1, Batch:  256, Loss: 0.595\n",
            "Epoch:  1, Batch:  288, Loss: 0.631\n",
            "Epoch:  1, Batch:  320, Loss: 0.627\n",
            "Epoch:  1, Batch:  352, Loss: 0.608\n",
            "Epoch:  1, Batch:  384, Loss: 0.685\n",
            "Epoch:  2, Batch:   32, Loss: 0.635\n",
            "Epoch:  2, Batch:   64, Loss: 0.607\n",
            "Epoch:  2, Batch:   96, Loss: 0.609\n",
            "Epoch:  2, Batch:  128, Loss: 0.592\n",
            "Epoch:  2, Batch:  160, Loss: 0.555\n",
            "Epoch:  2, Batch:  192, Loss: 0.611\n",
            "Epoch:  2, Batch:  224, Loss: 0.592\n",
            "Epoch:  2, Batch:  256, Loss: 0.584\n",
            "Epoch:  2, Batch:  288, Loss: 0.596\n",
            "Epoch:  2, Batch:  320, Loss: 0.592\n",
            "Epoch:  2, Batch:  352, Loss: 0.538\n",
            "Epoch:  2, Batch:  384, Loss: 0.516\n",
            "Epoch:  3, Batch:   32, Loss: 0.501\n",
            "Epoch:  3, Batch:   64, Loss: 0.523\n",
            "Epoch:  3, Batch:   96, Loss: 0.517\n",
            "Epoch:  3, Batch:  128, Loss: 0.508\n",
            "Epoch:  3, Batch:  160, Loss: 0.500\n",
            "Epoch:  3, Batch:  192, Loss: 0.506\n",
            "Epoch:  3, Batch:  224, Loss: 0.468\n",
            "Epoch:  3, Batch:  256, Loss: 0.436\n",
            "Epoch:  3, Batch:  288, Loss: 0.445\n",
            "Epoch:  3, Batch:  320, Loss: 0.463\n",
            "Epoch:  3, Batch:  352, Loss: 0.428\n",
            "Epoch:  3, Batch:  384, Loss: 0.455\n",
            "Epoch:  4, Batch:   32, Loss: 0.422\n",
            "Epoch:  4, Batch:   64, Loss: 0.453\n",
            "Epoch:  4, Batch:   96, Loss: 0.423\n",
            "Epoch:  4, Batch:  128, Loss: 0.415\n",
            "Epoch:  4, Batch:  160, Loss: 0.427\n",
            "Epoch:  4, Batch:  192, Loss: 0.426\n",
            "Epoch:  4, Batch:  224, Loss: 0.407\n",
            "Epoch:  4, Batch:  256, Loss: 0.412\n",
            "Epoch:  4, Batch:  288, Loss: 0.380\n",
            "Epoch:  4, Batch:  320, Loss: 0.390\n",
            "Epoch:  4, Batch:  352, Loss: 0.394\n",
            "Epoch:  4, Batch:  384, Loss: 0.383\n",
            "Epoch:  5, Batch:   32, Loss: 0.353\n",
            "Epoch:  5, Batch:   64, Loss: 0.380\n",
            "Epoch:  5, Batch:   96, Loss: 0.362\n",
            "Epoch:  5, Batch:  128, Loss: 0.398\n",
            "Epoch:  5, Batch:  160, Loss: 0.371\n",
            "Epoch:  5, Batch:  192, Loss: 0.402\n",
            "Epoch:  5, Batch:  224, Loss: 0.395\n",
            "Epoch:  5, Batch:  256, Loss: 0.359\n",
            "Epoch:  5, Batch:  288, Loss: 0.377\n",
            "Epoch:  5, Batch:  320, Loss: 0.365\n",
            "Epoch:  5, Batch:  352, Loss: 0.391\n",
            "Epoch:  5, Batch:  384, Loss: 0.343\n",
            "Epoch:  6, Batch:   32, Loss: 0.321\n",
            "Epoch:  6, Batch:   64, Loss: 0.350\n",
            "Epoch:  6, Batch:   96, Loss: 0.336\n",
            "Epoch:  6, Batch:  128, Loss: 0.376\n",
            "Epoch:  6, Batch:  160, Loss: 0.323\n",
            "Epoch:  6, Batch:  192, Loss: 0.361\n",
            "Epoch:  6, Batch:  224, Loss: 0.363\n",
            "Epoch:  6, Batch:  256, Loss: 0.351\n",
            "Epoch:  6, Batch:  288, Loss: 0.351\n",
            "Epoch:  6, Batch:  320, Loss: 0.323\n",
            "Epoch:  6, Batch:  352, Loss: 0.358\n",
            "Epoch:  6, Batch:  384, Loss: 0.341\n",
            "Epoch:  7, Batch:   32, Loss: 0.317\n",
            "Epoch:  7, Batch:   64, Loss: 0.339\n",
            "Epoch:  7, Batch:   96, Loss: 0.336\n",
            "Epoch:  7, Batch:  128, Loss: 0.328\n",
            "Epoch:  7, Batch:  160, Loss: 0.332\n",
            "Epoch:  7, Batch:  192, Loss: 0.335\n",
            "Epoch:  7, Batch:  224, Loss: 0.317\n",
            "Epoch:  7, Batch:  256, Loss: 0.322\n",
            "Epoch:  7, Batch:  288, Loss: 0.321\n",
            "Epoch:  7, Batch:  320, Loss: 0.305\n",
            "Epoch:  7, Batch:  352, Loss: 0.339\n",
            "Epoch:  7, Batch:  384, Loss: 0.320\n",
            "Epoch:  8, Batch:   32, Loss: 0.305\n",
            "Epoch:  8, Batch:   64, Loss: 0.298\n",
            "Epoch:  8, Batch:   96, Loss: 0.298\n",
            "Epoch:  8, Batch:  128, Loss: 0.313\n",
            "Epoch:  8, Batch:  160, Loss: 0.315\n",
            "Epoch:  8, Batch:  192, Loss: 0.338\n",
            "Epoch:  8, Batch:  224, Loss: 0.316\n",
            "Epoch:  8, Batch:  256, Loss: 0.319\n",
            "Epoch:  8, Batch:  288, Loss: 0.301\n",
            "Epoch:  8, Batch:  320, Loss: 0.314\n",
            "Epoch:  8, Batch:  352, Loss: 0.313\n",
            "Epoch:  8, Batch:  384, Loss: 0.297\n",
            "Epoch:  9, Batch:   32, Loss: 0.273\n",
            "Epoch:  9, Batch:   64, Loss: 0.276\n",
            "Epoch:  9, Batch:   96, Loss: 0.274\n",
            "Epoch:  9, Batch:  128, Loss: 0.274\n",
            "Epoch:  9, Batch:  160, Loss: 0.292\n",
            "Epoch:  9, Batch:  192, Loss: 0.304\n",
            "Epoch:  9, Batch:  224, Loss: 0.284\n",
            "Epoch:  9, Batch:  256, Loss: 0.285\n",
            "Epoch:  9, Batch:  288, Loss: 0.270\n",
            "Epoch:  9, Batch:  320, Loss: 0.275\n",
            "Epoch:  9, Batch:  352, Loss: 0.284\n",
            "Epoch:  9, Batch:  384, Loss: 0.278\n",
            "Epoch: 10, Batch:   32, Loss: 0.250\n",
            "Epoch: 10, Batch:   64, Loss: 0.240\n",
            "Epoch: 10, Batch:   96, Loss: 0.272\n",
            "Epoch: 10, Batch:  128, Loss: 0.268\n",
            "Epoch: 10, Batch:  160, Loss: 0.264\n",
            "Epoch: 10, Batch:  192, Loss: 0.227\n",
            "Epoch: 10, Batch:  224, Loss: 0.281\n",
            "Epoch: 10, Batch:  256, Loss: 0.269\n",
            "Epoch: 10, Batch:  288, Loss: 0.270\n",
            "Epoch: 10, Batch:  320, Loss: 0.235\n",
            "Epoch: 10, Batch:  352, Loss: 0.292\n",
            "Epoch: 10, Batch:  384, Loss: 0.282\n",
            "Saved model\n",
            "Classification accuracy: 86.01152368758002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW1950Woh0_g",
        "colab_type": "text"
      },
      "source": [
        "LSTM WITH ONLY BR REMOVED\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No5BMjprpTYE",
        "colab_type": "code",
        "outputId": "0f655385-5eed-4a5e-a96a-81a3ec1131a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as tnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as topti\n",
        "from torchtext import data\n",
        "from torchtext.vocab import GloVe\n",
        "from imdb_dataloader import IMDB\n",
        "import string\n",
        "\n",
        "\n",
        "# Class for creating the neural network.\n",
        "class Network(tnn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "  #       n_filters = 100\n",
        "  #       filter_sizes = [3,4,5]\n",
        "  #       output_dim = 1\n",
        "  #       dropout = 0.5\n",
        "  #       self.embedding = tnn.Embedding(133418, 100, padding_idx = 1)\n",
        "        \n",
        "  #       self.convs = tnn.ModuleList([\n",
        "  #                                   tnn.Conv1d(in_channels = 50, \n",
        "  #                                             out_channels = 100, \n",
        "  #                                             kernel_size = fs)\n",
        "  #                                   for fs in filter_sizes\n",
        "  #                                   ])\n",
        "        \n",
        "  #       self.fc = tnn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "  #       self.dropout = tnn.Dropout(dropout)\n",
        "        \n",
        "  # def forward(self, input,length):\n",
        "      \n",
        "  #     #text = [batch size, sent len]\n",
        "  #     print(input)\n",
        "  #     embedded = input\n",
        "              \n",
        "  #     #embedded = [batch size, sent len, emb dim]\n",
        "      \n",
        "  #     embedded = embedded.permute(0, 2, 1)\n",
        "      \n",
        "  #     #embedded = [batch size, emb dim, sent len]\n",
        "      \n",
        "  #     conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "          \n",
        "  #     #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "      \n",
        "  #     pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "      \n",
        "  #     #pooled_n = [batch size, n_filters]\n",
        "      \n",
        "  #     cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "  #     output = self.fc(cat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        embedding_dim = 50\n",
        "        hidden_dim = 256\n",
        "        output_dim = 1\n",
        "        n_layers = 2\n",
        "        bidirectional = True\n",
        "        dropout = 0.5\n",
        "\n",
        "        # self.embedding = tnn.Embedding(133418, embedding_dim, padding_idx = 1).float()\n",
        "        \n",
        "        self.rnn = tnn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,batch_first = True)\n",
        "        \n",
        "        self.fc = tnn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = tnn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, input, lengths):\n",
        "      # print(input.size())\n",
        "      #text = [sent len, batch size]\n",
        "      # input = input.permute(0,2,1)\n",
        "      embedded = input\n",
        "      # embedded = self.dropout(self.embedding(text))\n",
        "      \n",
        "      #embedded = [sent len, batch size, emb dim]\n",
        "      \n",
        "      #pack sequence\n",
        "      packed_embedded = tnn.utils.rnn.pack_padded_sequence(embedded, lengths,batch_first = True)\n",
        "      \n",
        "      packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "      # packed_output, hidden = self.rnn(packed_embedded)\n",
        "      #unpack sequence\n",
        "      output, output_lengths = tnn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "      #output = [sent len, batch size, hid dim * num directions]\n",
        "      #output over padding tokens are zero tensors\n",
        "      \n",
        "      #hidden = [num layers * num directions, batch size, hid dim]\n",
        "      #cell = [num layers * num directions, batch size, hid dim]\n",
        "      \n",
        "      #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "      #and apply dropout\n",
        "      \n",
        "      hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "              \n",
        "      #hidden = [batch size, hid dim * num directions]\n",
        "          \n",
        "      output=   self.fc(hidden) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #       self.relu = tnn.ReLU()\n",
        "  #       self.max_pool =tnn.MaxPool1d(kernel_size = 4)\n",
        "  #       self.conv_1 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.conv_2 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.conv_3 = tnn.Conv1d(in_channels = 50,\n",
        "  #                             out_channels = 50,\n",
        "  #                             kernel_size = 8,\n",
        "  #                             padding = 5)\n",
        "  #       self.mp = tnn.AdaptiveMaxPool1d(output_size = 1)\n",
        "  #       self.fc = tnn.Linear(50, 1)\n",
        "  # def forward(self, input, length):\n",
        "  #     \"\"\"\n",
        "  #     DO NOT MODIFY FUNCTION SIGNATURE\n",
        "  #     TODO:\n",
        "  #     Create the forward pass through the network.\n",
        "  #     \"\"\"\n",
        "  # #        print(length)\n",
        "  # #        print(input.shape)\n",
        "  #     #batch_size, seq_length, ip_dim = input.shape\n",
        "  #     #x = input.view([batch_size,ip_dim, seq_length])\n",
        "  #     x = input.permute(0,2,1)\n",
        "  #     x= self.conv_1(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x= self.max_pool(x)\n",
        "  #     x= self.conv_2(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x= self.max_pool(x) \n",
        "  #     x= self.conv_3(x)\n",
        "  #     x = self.relu(x)\n",
        "  #     x = self.mp(x)\n",
        "  #     #x= x.view(batch_size, -1)\n",
        "  #     x = x.squeeze(-1)\n",
        "  #     output = self.fc(x)\n",
        "      output = output[:,-1]\n",
        "      return output\n",
        "        \n",
        "\n",
        "\n",
        "class PreProcessing():\n",
        "    def pre(x):\n",
        "        \"\"\"Called after tokenization\"\"\"\n",
        "        # print(x)\n",
        "        #Remove Punctuations\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        tokens = [w.translate(table) for w in x]\n",
        "        #Remove numbers\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        #Removing common stop words\n",
        "        # TO DO  - Would removing stop words make a difference ? \n",
        "        # stop_words = ['br','a','about','after','again','against','ain','all','am','an','and','any','are','aren','arent','as','at','be','because','been','before','being','both','but','by','can','couldn','couldnt','d','did','didn','didnt','do','does','doesn','doesnt','doing','don','dont','down','during','each','few','for','from','further','had','hadn','hadnt','has','hasn','hasnt','have','haven','havent','having','he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','isn','isnt','it','its','its','itself','just','ll','m','ma','me','mightn','mightnt','more','most','mustn','mustnt','my','myself','needn','neednt','no','nor','not','now','o','of','off','on','once','only','or','other','our','ours','ourselves','out','over','own','re','s','same','shan','shant','she','shes','should','shouldve','shouldn','shouldnt','so','some','such','t','than','that','thatll','the','their','theirs','them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','ve','very','was','wasn','wasnt','we','were','weren','werent','what','when','where','which','while','who','whom','why','will','with','won','wont','wouldn','y','you','youd','youll','youre','youve','your','yours','yourself','yourselves','could','hed','hell','hes','heres','hows','id','ill','im','ive','lets','shed','shell','thats','theres','theyd','theyll','theyre','theyve','wed','well','were','weve','whats','whens','whos','whys','would'] \n",
        "        stop_words = ['br']\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        #removing alphabets\n",
        "        x = [word for word in tokens if len(word) > 1]\n",
        "        return x\n",
        "\n",
        "    def post(batch, vocab):\n",
        "        \"\"\"Called after numericalization but prior to vectorization\"\"\"\n",
        "      \n",
        "        \n",
        "        return batch, vocab\n",
        "\n",
        "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=None)\n",
        "\n",
        "\n",
        "def lossFunc():\n",
        "    \"\"\"\n",
        "    Define a loss function appropriate for the above networks that will\n",
        "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
        "    \"\"\"\n",
        "    return tnn.BCEWithLogitsLoss()\n",
        "\n",
        "def main():\n",
        "    # Use a GPU if available, as it should be faster.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device: \" + str(device))\n",
        "\n",
        "    # Load the training dataset, and create a data loader to generate a batch.\n",
        "    textField = PreProcessing.text_field\n",
        "    labelField = data.Field(sequential=False)\n",
        "\n",
        "    train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
        "\n",
        "    textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
        "    labelField.build_vocab(train, dev)\n",
        "    print(\"Input_DIM = \" , len(textField.vocab))\n",
        "    print(\"pad_idx = \" ,  textField.vocab.stoi[textField.pad_token])\n",
        "\n",
        "    trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
        "                                                         sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
        "    \n",
        "    net = Network().to(device)\n",
        "    criterion =lossFunc()\n",
        "    optimiser = topti.Adam(net.parameters(), lr=0.001)  # Minimise the loss using the Adam algorithm.\n",
        "\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(trainLoader):\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "\n",
        "            labels -= 1\n",
        "\n",
        "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
        "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            # Forward pass through the network.\n",
        "            output = net(inputs, length)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            # Calculate gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Minimise the loss according to the gradient.\n",
        "            optimiser.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 32 == 31:\n",
        "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
        "                running_loss = 0\n",
        "\n",
        "    num_correct = 0\n",
        "\n",
        "    # Save mode\n",
        "    torch.save(net.state_dict(), \"./model.pth\")\n",
        "    print(\"Saved model\")\n",
        "\n",
        "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
        "    # computations and reduce memory usage.\n",
        "    with torch.no_grad():\n",
        "        for batch in testLoader:\n",
        "            # Get a batch and potentially send it to GPU memory.\n",
        "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
        "                device), batch.label.type(torch.FloatTensor).to(device)\n",
        "\n",
        "            labels -= 1\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = torch.sigmoid(net(inputs, length))\n",
        "            predicted = torch.round(outputs)\n",
        "\n",
        "            num_correct += torch.sum(labels == predicted).item()\n",
        "\n",
        "    accuracy = 100 * num_correct / len(dev)\n",
        "\n",
        "    print(f\"Classification accuracy: {accuracy}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Input_DIM =  133418\n",
            "pad_idx =  1\n",
            "Epoch:  1, Batch:   32, Loss: 0.688\n",
            "Epoch:  1, Batch:   64, Loss: 0.668\n",
            "Epoch:  1, Batch:   96, Loss: 0.686\n",
            "Epoch:  1, Batch:  128, Loss: 0.688\n",
            "Epoch:  1, Batch:  160, Loss: 0.676\n",
            "Epoch:  1, Batch:  192, Loss: 0.668\n",
            "Epoch:  1, Batch:  224, Loss: 0.633\n",
            "Epoch:  1, Batch:  256, Loss: 0.635\n",
            "Epoch:  1, Batch:  288, Loss: 0.646\n",
            "Epoch:  1, Batch:  320, Loss: 0.648\n",
            "Epoch:  1, Batch:  352, Loss: 0.635\n",
            "Epoch:  1, Batch:  384, Loss: 0.642\n",
            "Epoch:  2, Batch:   32, Loss: 0.611\n",
            "Epoch:  2, Batch:   64, Loss: 0.636\n",
            "Epoch:  2, Batch:   96, Loss: 0.636\n",
            "Epoch:  2, Batch:  128, Loss: 0.686\n",
            "Epoch:  2, Batch:  160, Loss: 0.672\n",
            "Epoch:  2, Batch:  192, Loss: 0.675\n",
            "Epoch:  2, Batch:  224, Loss: 0.672\n",
            "Epoch:  2, Batch:  256, Loss: 0.666\n",
            "Epoch:  2, Batch:  288, Loss: 0.669\n",
            "Epoch:  2, Batch:  320, Loss: 0.659\n",
            "Epoch:  2, Batch:  352, Loss: 0.650\n",
            "Epoch:  2, Batch:  384, Loss: 0.656\n",
            "Epoch:  3, Batch:   32, Loss: 0.658\n",
            "Epoch:  3, Batch:   64, Loss: 0.659\n",
            "Epoch:  3, Batch:   96, Loss: 0.649\n",
            "Epoch:  3, Batch:  128, Loss: 0.636\n",
            "Epoch:  3, Batch:  160, Loss: 0.650\n",
            "Epoch:  3, Batch:  192, Loss: 0.632\n",
            "Epoch:  3, Batch:  224, Loss: 0.655\n",
            "Epoch:  3, Batch:  256, Loss: 0.619\n",
            "Epoch:  3, Batch:  288, Loss: 0.613\n",
            "Epoch:  3, Batch:  320, Loss: 0.576\n",
            "Epoch:  3, Batch:  352, Loss: 0.549\n",
            "Epoch:  3, Batch:  384, Loss: 0.562\n",
            "Epoch:  4, Batch:   32, Loss: 0.492\n",
            "Epoch:  4, Batch:   64, Loss: 0.447\n",
            "Epoch:  4, Batch:   96, Loss: 0.481\n",
            "Epoch:  4, Batch:  128, Loss: 0.438\n",
            "Epoch:  4, Batch:  160, Loss: 0.454\n",
            "Epoch:  4, Batch:  192, Loss: 0.422\n",
            "Epoch:  4, Batch:  224, Loss: 0.413\n",
            "Epoch:  4, Batch:  256, Loss: 0.476\n",
            "Epoch:  4, Batch:  288, Loss: 0.384\n",
            "Epoch:  4, Batch:  320, Loss: 0.417\n",
            "Epoch:  4, Batch:  352, Loss: 0.407\n",
            "Epoch:  4, Batch:  384, Loss: 0.395\n",
            "Epoch:  5, Batch:   32, Loss: 0.407\n",
            "Epoch:  5, Batch:   64, Loss: 0.364\n",
            "Epoch:  5, Batch:   96, Loss: 0.370\n",
            "Epoch:  5, Batch:  128, Loss: 0.409\n",
            "Epoch:  5, Batch:  160, Loss: 0.403\n",
            "Epoch:  5, Batch:  192, Loss: 0.380\n",
            "Epoch:  5, Batch:  224, Loss: 0.362\n",
            "Epoch:  5, Batch:  256, Loss: 0.364\n",
            "Epoch:  5, Batch:  288, Loss: 0.367\n",
            "Epoch:  5, Batch:  320, Loss: 0.383\n",
            "Epoch:  5, Batch:  352, Loss: 0.356\n",
            "Epoch:  5, Batch:  384, Loss: 0.352\n",
            "Epoch:  6, Batch:   32, Loss: 0.346\n",
            "Epoch:  6, Batch:   64, Loss: 0.350\n",
            "Epoch:  6, Batch:   96, Loss: 0.362\n",
            "Epoch:  6, Batch:  128, Loss: 0.348\n",
            "Epoch:  6, Batch:  160, Loss: 0.344\n",
            "Epoch:  6, Batch:  192, Loss: 0.358\n",
            "Epoch:  6, Batch:  224, Loss: 0.371\n",
            "Epoch:  6, Batch:  256, Loss: 0.360\n",
            "Epoch:  6, Batch:  288, Loss: 0.319\n",
            "Epoch:  6, Batch:  320, Loss: 0.364\n",
            "Epoch:  6, Batch:  352, Loss: 0.347\n",
            "Epoch:  6, Batch:  384, Loss: 0.330\n",
            "Epoch:  7, Batch:   32, Loss: 0.328\n",
            "Epoch:  7, Batch:   64, Loss: 0.335\n",
            "Epoch:  7, Batch:   96, Loss: 0.303\n",
            "Epoch:  7, Batch:  128, Loss: 0.322\n",
            "Epoch:  7, Batch:  160, Loss: 0.342\n",
            "Epoch:  7, Batch:  192, Loss: 0.323\n",
            "Epoch:  7, Batch:  224, Loss: 0.317\n",
            "Epoch:  7, Batch:  256, Loss: 0.333\n",
            "Epoch:  7, Batch:  288, Loss: 0.316\n",
            "Epoch:  7, Batch:  320, Loss: 0.316\n",
            "Epoch:  7, Batch:  352, Loss: 0.333\n",
            "Epoch:  7, Batch:  384, Loss: 0.318\n",
            "Epoch:  8, Batch:   32, Loss: 0.306\n",
            "Epoch:  8, Batch:   64, Loss: 0.310\n",
            "Epoch:  8, Batch:   96, Loss: 0.313\n",
            "Epoch:  8, Batch:  128, Loss: 0.288\n",
            "Epoch:  8, Batch:  160, Loss: 0.323\n",
            "Epoch:  8, Batch:  192, Loss: 0.314\n",
            "Epoch:  8, Batch:  224, Loss: 0.323\n",
            "Epoch:  8, Batch:  256, Loss: 0.303\n",
            "Epoch:  8, Batch:  288, Loss: 0.317\n",
            "Epoch:  8, Batch:  320, Loss: 0.327\n",
            "Epoch:  8, Batch:  352, Loss: 0.343\n",
            "Epoch:  8, Batch:  384, Loss: 0.310\n",
            "Epoch:  9, Batch:   32, Loss: 0.286\n",
            "Epoch:  9, Batch:   64, Loss: 0.292\n",
            "Epoch:  9, Batch:   96, Loss: 0.292\n",
            "Epoch:  9, Batch:  128, Loss: 0.294\n",
            "Epoch:  9, Batch:  160, Loss: 0.287\n",
            "Epoch:  9, Batch:  192, Loss: 0.317\n",
            "Epoch:  9, Batch:  224, Loss: 0.266\n",
            "Epoch:  9, Batch:  256, Loss: 0.293\n",
            "Epoch:  9, Batch:  288, Loss: 0.298\n",
            "Epoch:  9, Batch:  320, Loss: 0.294\n",
            "Epoch:  9, Batch:  352, Loss: 0.285\n",
            "Epoch:  9, Batch:  384, Loss: 0.300\n",
            "Epoch: 10, Batch:   32, Loss: 0.254\n",
            "Epoch: 10, Batch:   64, Loss: 0.253\n",
            "Epoch: 10, Batch:   96, Loss: 0.284\n",
            "Epoch: 10, Batch:  128, Loss: 0.266\n",
            "Epoch: 10, Batch:  160, Loss: 0.277\n",
            "Epoch: 10, Batch:  192, Loss: 0.238\n",
            "Epoch: 10, Batch:  224, Loss: 0.273\n",
            "Epoch: 10, Batch:  256, Loss: 0.289\n",
            "Epoch: 10, Batch:  288, Loss: 0.295\n",
            "Epoch: 10, Batch:  320, Loss: 0.294\n",
            "Epoch: 10, Batch:  352, Loss: 0.283\n",
            "Epoch: 10, Batch:  384, Loss: 0.275\n",
            "Saved model\n",
            "Classification accuracy: 85.56338028169014\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}