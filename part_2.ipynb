{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denzilsaldanha/neural-nets-course/blob/master/part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzzAV3mQBpAr",
        "colab_type": "code",
        "outputId": "20580078-4de2-4343-d958-4f97e1b8e7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "part2.py\n",
        "\n",
        "UNSW COMP9444 Neural Networks and Deep Learning\n",
        "\n",
        "ONLY COMPLETE METHODS AND CLASSES MARKED \"TODO\".\n",
        "\n",
        "DO NOT MODIFY IMPORTS. DO NOT ADD EXTRA FUNCTIONS.\n",
        "DO NOT MODIFY EXISTING FUNCTION SIGNATURES.\n",
        "DO NOT IMPORT ADDITIONAL LIBRARIES.\n",
        "DOING SO MAY CAUSE YOUR CODE TO FAIL AUTOMATED TESTING.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class LinearModel:\n",
        "    def __init__(self, num_inputs, learning_rate):\n",
        "        \"\"\"\n",
        "        Model is very similar to the Perceptron shown in Lectures 1c, slide 12, except that:\n",
        "        (1) the bias is indexed by w(n+1) rather than w(0), and\n",
        "        (2) the activation function is a (continuous) sigmoid rather than a (discrete) step function.\n",
        "\n",
        "        x1 ----> * w1 ----\\\n",
        "        x2 ----> * w2 -----\\\n",
        "        x3 ----> * w3 ------\\\n",
        "        ...\n",
        "                             \\\n",
        "        xn ----> * wn -------+--> s --> activation ---> z\n",
        "        1  ----> * w(n+1) --/\n",
        "        \"\"\"\n",
        "        self.num_inputs = num_inputs\n",
        "        self.lr = learning_rate\n",
        "        self.weights = np.asarray([1.0, -1.0, 0.0])  # Initialize as straight line\n",
        "\n",
        "    def activation(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Implement a sigmoid activation function that accepts a float and returns\n",
        "        a float, but raises a Value error if a boolean, list or numpy array is passed in\n",
        "        hint: consider np.exp()\n",
        "        \"\"\"\n",
        "        if isinstance(x, float) : \n",
        "          return 1/(1+np.exp(-x))\n",
        "        else:\n",
        "          raise ValueError(\"Wrong value sent\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        TODO: Implement the forward pass (inference) of a the model.\n",
        "\n",
        "        inputs is a numpy array. The bias term is the last element in self.weights.\n",
        "        hint: call the activation function you have implemented above.\n",
        "        \"\"\"\n",
        "        \n",
        "        return  self.activation(np.dot(inputs, self.weights[0:-1]) + self.weights[-1])\n",
        "#         z = np.dot(inputs,self.weights[0:-1]) + self.weights[-1]\n",
        "#         return self.activation(z)\n",
        "#         k0 = inputs\n",
        "# #         print(np.dot(k0,self.weights[0]))\n",
        "#         k1 = self.activation(np.dot(k0,self.weights[0]))\n",
        "#         k2 = self.activation(np.dot(k1,self.weights[1]))\n",
        "#         return k2 + self.weights[-1]\n",
        "       \n",
        "\n",
        "    @staticmethod\n",
        "    def loss(prediction, label):\n",
        "        \"\"\"\n",
        "        TODO: Return the cross entropy for the given prediction and label\n",
        "        hint: consider using np.log()\n",
        "        \"\"\"\n",
        "        return - (np.multiply(label, np.log(prediction)) + np.multiply((1-label), np.log(1-prediction)))\n",
        "\n",
        "    @staticmethod\n",
        "    def error(prediction, label):\n",
        "        \"\"\"\n",
        "        TODO: Return the difference between the label and the prediction\n",
        "\n",
        "        For example, if label= 1 and the prediction was 0.8, return 0.2\n",
        "                     if label= 0 and the preduction was 0.43 return -0.43\n",
        "        \"\"\"\n",
        "        return label - prediction\n",
        "\n",
        "    def backward(self, inputs, diff):\n",
        "        \"\"\"\n",
        "        TODO: Adjust self.weights by gradient descent\n",
        "\n",
        "        We take advantage of the simplification shown in Lecture 2b, slide 23,\n",
        "        to compute the gradient directly from the differential or difference\n",
        "        -dE/ds = t - z (which is passed in as diff)\n",
        "\n",
        "        The resulting weight update should look essentially the same as for the\n",
        "        Perceptron Learning Rule (shown in Lectures 1c, slide 11) except that\n",
        "        the error can take on any continuous value between -1 and +1,\n",
        "        rather than being restricted to the integer values -1, 0 or +1.\n",
        "\n",
        "        Note: Numpy arrays are passed by reference and can be modified in-place\n",
        "        \"\"\"\n",
        "        \n",
        "#         z = np.dot(inputs,self.weights[0:-1]) +self.weights[-1]\n",
        "#         z1 = self.activation(z)\n",
        "#         z2 =  self.activation(z1) * (1 - self.activation(z1))\n",
        "#         gd = z2 * inputs *diff\n",
        "       \n",
        "  \n",
        "#         res=self.activation(np.matmul(inputs, self.weights[0:-1]) + self.weights[-1])\n",
        "#         res1 = self.activation(res) *(1-self.activation(res))\n",
        "# #         res = res/(1-res)\n",
        "#         gd =diff * res1\n",
        "#         self.weights[0:2] += self.lr *np.dot(inputs.T, gd) \n",
        "        self.weights[0:2] += self.lr * inputs * diff\n",
        "        self.weights[-1] += self.lr *diff\n",
        "\n",
        "          \n",
        "        \n",
        "    def plot(self, inputs, marker):\n",
        "        \"\"\"\n",
        "        Plot the data and the decision boundary\n",
        "        \"\"\"\n",
        "        xmin = inputs[:, 0].min() * 1.1\n",
        "        xmax = inputs[:, 0].max() * 1.1\n",
        "        ymin = inputs[:, 1].min() * 1.1\n",
        "        ymax = inputs[:, 1].max() * 1.1\n",
        "\n",
        "        x = np.arange(xmin * 1.3, xmax * 1.3, 0.1)\n",
        "        plt.scatter(inputs[:25, 0], inputs[:25, 1], c=\"C0\", edgecolors='w', s=100)\n",
        "        plt.scatter(inputs[25:, 0], inputs[25:, 1], c=\"C1\", edgecolors='w', s=100)\n",
        "\n",
        "        plt.xlim((xmin, xmax))\n",
        "        plt.ylim((ymin, ymax))\n",
        "        plt.plot(x, -(self.weights[0] * x + self.weights[2]) / self.weights[1], marker, alpha=0.6)\n",
        "        plt.title(\"Data and decision boundary\")\n",
        "        plt.xlabel(\"x1\")\n",
        "        plt.ylabel(\"x2\").set_rotation(0)\n",
        "\n",
        "\n",
        "def main():\n",
        "    inputs, labels = pkl.load(open(\"data/binary_classification_data.pkl\", \"rb\"))\n",
        "\n",
        "    epochs = 400\n",
        "    model = LinearModel(num_inputs=inputs.shape[1], learning_rate=0.01)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        num_correct = 0\n",
        "        cost = 0\n",
        "        for x, y in zip(inputs, labels):\n",
        "            # Get prediction\n",
        "            output = model.forward(x)\n",
        "\n",
        "            # Calculate loss\n",
        "            cost += model.loss(output, y)\n",
        "\n",
        "            # Calculate difference or differential\n",
        "            diff = model.error(output, y)\n",
        "\n",
        "            # Update the weights\n",
        "            model.backward(x, diff)\n",
        "\n",
        "            # Record accuracy\n",
        "            preds = output > 0.5  # 0.5 is midline of sigmoid\n",
        "            num_correct += int(preds == y)\n",
        "\n",
        "        print(f\" Cost: {cost/len(inputs):.2f} Accuracy: {num_correct / len(inputs) * 100:.2f}%\")\n",
        "        model.plot(inputs, \"C2--\")\n",
        "    model.plot(inputs, \"k\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9fb8514b930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-b9fb8514b930>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/binary_classification_data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/binary_classification_data.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1aGJZtIECtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "weights = np.asarray([1.0, -1.0, 0.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FClIvP_ED9m",
        "colab_type": "code",
        "outputId": "4f2130cf-137e-4798-ad13-17823779b545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weights[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZv-Mg_lEK2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a= np.array([1,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9aX7T3PEOYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b= np.array([1,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSFcOyZUksqe",
        "colab_type": "code",
        "outputId": "beedff2f-949a-4060-86fa-306dcf7aa16a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.matmul(a,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Ume4OSkuVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}